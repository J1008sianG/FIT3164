L <- as.data.frame(c(1:49))
set.seed(31842305) # Your Student ID is the random seed
L <- L[sample(nrow(L), 10, replace = FALSE),] # sample 10 locations
WAUS <- WAUS[(WAUS$Location %in% L),]
WAUS <- WAUS[sample(nrow(WAUS), 2000, replace = FALSE),] # sample 2000 rows
WAUS = na.omit(WAUS)
WAUS$WarmerTomorrow = as.factor(WAUS$WarmerTomorrow)
warm_tmr = as.data.frame(table(WAUS$WarmerTomorrow))
warmer =  round((warm_tmr[2,2] / sum(warm_tmr[,2])) * 100, digits = 2) #52.43
cooler = round((warm_tmr[1,2] / sum(warm_tmr[,2])) * 100, digits = 2) # 47.57
df_summary <- subset(WAUS, select = -c(1,2,3,4,10,12,13,24))
df_summary = round(stat.desc(df_summary), digits = 2)
df_summary
set.seed(31842305)
train.row = sample(1:nrow(WAUS), 0.7*nrow(WAUS))
train.data = WAUS[train.row,]
test.data = WAUS[-train.row,]
tree.fit = tree(WarmerTomorrow ~., data = train.data)
tree.pred = predict(tree.fit, test.data, type = "class")
naive.fit = naiveBayes(WarmerTomorrow ~., data = train.data)
naive.predict = predict(naive.fit, test.data)
wbag = bagging(WarmerTomorrow ~., train.data, mfinal = 10)
wbag_pred = predict.bagging(wbag, test.data)
wboost = boosting(WarmerTomorrow ~., train.data, mfinal = 10)
wboost_pred = predict.boosting(wboost, test.data)
wrf = randomForest(WarmerTomorrow ~., train.data)
wrf_pred = predict(wrf, test.data)
tree.pred = predict(tree.fit, test.data, type = "class")
Naïve Bayes
```{r}
naive.predict = predict(naive.fit, test.data)
wbag_pred = predict.bagging(wbag, test.data)
wboost_pred = predict.boosting(wboost, test.data)
wrf_pred = predict(wrf, test.data)
naive.pred = predict(naive.fit, test.data)
tree.cfm = table(actual = test.data$WarmerTomorrow, predicted = tree.pred)
tree.cfm
round(mean(tree.pred == test.data$WarmerTomorrow)*100, digits = 2)
tree.acc = round(mean(tree.pred == test.data$WarmerTomorrow)*100, digits = 2)
cat("Decision Tree accuracy is: ", tree.acc, "%")
naive.cfm = table(actual = test.data$WarmerTomorrow, predicted = naive.pred)
naive.cfm
naive.acc = round(mean(tree.pred == test.data$WarmerTomorrow)*100, digits = 2)
naive.acc
naive.pred == test.data$WarmerTomorrow
naive.acc = round(mean(naive.pred == test.data$WarmerTomorrow)*100, digits = 2)
naive.acc
cat("Naïve Bayes model accuracy is: ", naive.acc, "%")
wbag.cfm = wbag_pred$confusion
wbag.cfm
wbag.acc = round(mean(wbag_pred == test.data$WarmerTomorrow)*100, digits = 2)
cat("Bagging ensemble model accuracy is: ", wbag.acc, "%")
wbag.acc = round(mean(wbag_pred$class == test.data$WarmerTomorrow)*100, digits = 2)
cat("Bagging ensemble model accuracy is: ", wbag.acc, "%")
wboost.cfm = wboost_pred$confusion
wboost.cfm
wboost.acc = round(mean(wboost_pred$class == test.data$WarmerTomorrow)*100, digits = 2)
cat("Boosting ensemble model accuracy is: ", wboost.acc, "%")
wrf_pred = predict(wrf, test.data)
wrf.cfm = wrf_pred$confusion
wrf_pred
wrf.cfm = tree.cfm = table(actual = test.data$WarmerTomorrow, predicted = wrf_pred)
wboost$class
cat("Boosting ensemble model accuracy is: ", wboost.acc, "%")
wrf.cfm = tree.cfm = table(actual = test.data$WarmerTomorrow, predicted = wrf_pred)
wrf.acc = round(mean(wrf_pred == test.data$WarmerTomorrow)*100, digits = 2)
wrf.cfm
cat("Random Forest ensemble model accuracy is: ", wrf.acc, "%")
#load libraries
library(pastecs)
library(tree)
library(rpart)
library(e1071)
library(adabag)
library(randomForest)
rm(list = ls())
WAUS <- read.csv("WarmerTomorrow2022.csv", stringsAsFactors = T)
L <- as.data.frame(c(1:49))
set.seed(31842305) # Your Student ID is the random seed
L <- L[sample(nrow(L), 10, replace = FALSE),] # sample 10 locations
WAUS <- WAUS[(WAUS$Location %in% L),]
WAUS <- WAUS[sample(nrow(WAUS), 2000, replace = FALSE),] # sample 2000 rows
WAUS = na.omit(WAUS)
WAUS$WarmerTomorrow = as.factor(WAUS$WarmerTomorrow)
warm_tmr = as.data.frame(table(WAUS$WarmerTomorrow))
warmer =  round((warm_tmr[2,2] / sum(warm_tmr[,2])) * 100, digits = 2) #52.43
cooler = round((warm_tmr[1,2] / sum(warm_tmr[,2])) * 100, digits = 2) # 47.57
df_summary <- subset(WAUS, select = -c(1,2,3,4,10,12,13,24))
df_summary = round(stat.desc(df_summary), digits = 2)
df_summary
set.seed(31842305)
train.row = sample(1:nrow(WAUS), 0.7*nrow(WAUS))
train.data = WAUS[train.row,]
test.data = WAUS[-train.row,]
tree.fit = tree(WarmerTomorrow ~., data = train.data)
naive.fit = naiveBayes(WarmerTomorrow ~., data = train.data)
wbag = bagging(WarmerTomorrow ~., train.data, mfinal = 10)
wboost = boosting(WarmerTomorrow ~., train.data, mfinal = 10)
wrf = randomForest(WarmerTomorrow ~., train.data)
tree.pred = predict(tree.fit, test.data, type = "class")
#confusion matrix
tree.cfm = table(actual = test.data$WarmerTomorrow, predicted = tree.pred)
tree.acc = round(mean(tree.pred == test.data$WarmerTomorrow)*100, digits = 2)
cat("Decision Tree accuracy is: ", tree.acc, "%")
naive.pred = predict(naive.fit, test.data)
#confusion matrix
naive.cfm = table(actual = test.data$WarmerTomorrow, predicted = naive.pred)
naive.acc = round(mean(naive.pred == test.data$WarmerTomorrow)*100, digits = 2)
cat("Naïve Bayes model accuracy is: ", naive.acc, "%")
wbag_pred = predict.bagging(wbag, test.data)
#confusion matrix
wbag.cfm = wbag_pred$confusion
wbag.acc = round(mean(wbag_pred$class == test.data$WarmerTomorrow)*100, digits = 2)
cat("Bagging ensemble model accuracy is: ", wbag.acc, "%")
wboost_pred = predict.boosting(wboost, test.data)
#confusion matrix
wboost.cfm = wboost_pred$confusion
wboost.acc = round(mean(wboost_pred$class == test.data$WarmerTomorrow)*100, digits = 2)
cat("Boosting ensemble model accuracy is: ", wboost.acc, "%")
wrf_pred = predict(wrf, test.data)
#confusion matrix
wrf.cfm = tree.cfm = table(actual = test.data$WarmerTomorrow, predicted = wrf_pred)
wrf.acc = round(mean(wrf_pred == test.data$WarmerTomorrow)*100, digits = 2)
cat("Random Forest ensemble model accuracy is: ", wrf.acc, "%")
install.packages("ROCR")
library(ROCR)
tree.prob.pred = predict(tree.fit, train.data, method = "class")
head(tree.prob.pred)
tree.conf = predict(tree.fit, train.data, method = "class")
head(tree.conf[,2])
tree.perf = performance(tree.conf.pred, "tpr", "fpr")
tree.conf.pred = prediction(tree.conf[,2], test.data$WarmerTomorrow)
tree.perf = performance(tree.conf.pred, "tpr", "fpr")
tree.conf = predict(tree.fit, train.data, method = "class")
tree.conf.pred = prediction(tree.conf[,2], test.data$WarmerTomorrow)
tree.conf = predict(tree.fit, test.data, type = "prob")
tree.conf = predict(tree.fit, test.data, type = "vector")
tree.conf.pred = prediction(tree.conf[,2], test.data$WarmerTomorrow)
tree.perf = performance(tree.conf.pred, "tpr", "fpr")
plot(tree.perf, main = "ROC curve for decision  tree")
abline(0,1)
plot(tree.perf, main = "ROC curve for decision  tree") + ablibne()
plot(tree.perf, main = "ROC curve for decision  tree") + abline()
plot(tree.perf, main = "ROC curve for decision  tree") + abline(0,1)
plot(tree.perf, main = "ROC curve for classifier") + abline(0,1)
tree.conf = predict(tree.fit, test.data, type = "prob")
vector
vector
tree.conf = predict(tree.fit, test.data, type = "vector")
tree.conf.pred = prediction(tree.conf[,2], test.data$WarmerTomorrow)
tree.perf = performance(tree.conf.pred, "tpr", "fpr")
plot(tree.perf, main = "ROC curve for classifier") + abline(0,1)
head(tree.conf)
tree.conf = predict(tree.fit, test.data, type = "vector")
tree.conf.pred = prediction(tree.conf[,2], test.data$WarmerTomorrow)
tree.perf = performance(tree.conf.pred, "tpr", "fpr")
plot(tree.perf, main = "ROC curve for classifier") + abline(0,1)
naive.conf = predict(naive.fit, test.data, type = "raw")
head(naive.conf)
naive.conf.pred = prediction(naive.conf[,2], test.data$WarmerTomorrow)
naive.perf = performance(naive.conf.pred, "tpr", "fpr")
naive.perf = performance(naive.conf.pred, "tpr", "fpr")
plot(naive.perf, add = TRUE, col = "blueviolet")
plot(naive.perf, add = TRUE)
naive.conf = predict(naive.fit, test.data, type = "raw")
naive.conf.pred = prediction(naive.conf[,2], test.data$WarmerTomorrow)
naive.perf = performance(naive.conf.pred, "tpr", "fpr")
plot(naive.perf, add = TRUE)
naive.perf = performance(naive.conf.pred, "tpr", "fpr")
naive.conf = predict(naive.fit, test.data, type = "raw")
naive.conf.pred = prediction(naive.conf[,2], test.data$WarmerTomorrow)
naive.perf = performance(naive.conf.pred, "tpr", "fpr")
```{r}
plot(naive.perf, add = TRUE, col = "blueviolet")
{plot(naive.perf, add = TRUE, col = "blueviolet")}
{plot(naive.perf, add = TRUE, col = "blueviolet")}
{plot(naive.perf, add = TRUE, col = "blueviolet")}
ggplot(naive.perf, add = TRUE, col = "red")
plot(naive.perf, add = TRUE, col = "red")
naive.perf
plot(naive.perf)
plot(naive.perf, add = TRUE)
plot(naive.perf, col = "blueviolet")
plot(naive.perf, col = "blueviolet", add = TRUE)
plot(naive.perf, col = "blueviolet", add = TRUE) + abline(0,1)
par(add=TRUE)
plot(naive.perf, col = "blueviolet")
plot(tree.perf, main = "ROC curve for classifier") + abline(0,1)
plot(naive.perf, col = "blueviolet", add = TRUE)
{plot(naive.perf, col = "blueviolet", add = TRUE), abline(0,1)}
{plot(naive.perf, col = "blueviolet", add = TRUE) abline(0,1)}
plot(naive.perf, col = "blueviolet", add = T)
install.packages("sf")
library(sf)
naive.conf = predict(naive.fit, test.data, type = "raw")
naive.conf.pred = prediction(naive.conf[,2], test.data$WarmerTomorrow)
naive.perf = performance(naive.conf.pred, "tpr", "fpr")
plot(naive.perf, col = "blueviolet", add = T)
plot(naive.perf, col = "blueviolet", add = TRUE)
```{r, fig.height=5}
tree.conf = predict(tree.fit, test.data, type = "vector")
tree.conf.pred = prediction(tree.conf[,2], test.data$WarmerTomorrow)
tree.perf = performance(tree.conf.pred, "tpr", "fpr")
plot(tree.perf, main = "ROC curve for classifier") + abline(0,1)
```{r}
tree.conf = predict(tree.fit, test.data, type = "vector")
tree.conf.pred = prediction(tree.conf[,2], test.data$WarmerTomorrow)
tree.perf = performance(tree.conf.pred, "tpr", "fpr")
plot(tree.perf, main = "ROC curve for classifier") + abline(0,1)
wbag_pred$prob[,2]
#load libraries
library(pastecs)
library(tree)
library(rpart)
library(e1071)
library(adabag)
library(randomForest)
library(ROCR)
library(sf)
rm(list = ls())
WAUS <- read.csv("WarmerTomorrow2022.csv", stringsAsFactors = T)
L <- as.data.frame(c(1:49))
set.seed(31842305) # Your Student ID is the random seed
L <- L[sample(nrow(L), 10, replace = FALSE),] # sample 10 locations
WAUS <- WAUS[(WAUS$Location %in% L),]
WAUS <- WAUS[sample(nrow(WAUS), 2000, replace = FALSE),] # sample 2000 rows
WAUS = na.omit(WAUS)
WAUS$WarmerTomorrow = as.factor(WAUS$WarmerTomorrow)
warm_tmr = as.data.frame(table(WAUS$WarmerTomorrow))
warmer =  round((warm_tmr[2,2] / sum(warm_tmr[,2])) * 100, digits = 2) #52.43
cooler = round((warm_tmr[1,2] / sum(warm_tmr[,2])) * 100, digits = 2) # 47.57
df_summary <- subset(WAUS, select = -c(1,2,3,4,10,12,13,24))
df_summary = round(stat.desc(df_summary), digits = 2)
df_summary
set.seed(31842305)
train.row = sample(1:nrow(WAUS), 0.7*nrow(WAUS))
train.data = WAUS[train.row,]
test.data = WAUS[-train.row,]
tree.fit = tree(WarmerTomorrow ~., data = train.data)
naive.fit = naiveBayes(WarmerTomorrow ~., data = train.data)
wbag = bagging(WarmerTomorrow ~., train.data, mfinal = 10)
wboost = boosting(WarmerTomorrow ~., train.data, mfinal = 10)
wrf = randomForest(WarmerTomorrow ~., train.data)
tree.pred = predict(tree.fit, test.data, type = "class")
#confusion matrix
tree.cfm = table(actual = test.data$WarmerTomorrow, predicted = tree.pred)
tree.acc = round(mean(tree.pred == test.data$WarmerTomorrow)*100, digits = 2)
cat("Decision Tree accuracy is: ", tree.acc, "%")
naive.pred = predict(naive.fit, test.data)
#confusion matrix
naive.cfm = table(actual = test.data$WarmerTomorrow, predicted = naive.pred)
naive.acc = round(mean(naive.pred == test.data$WarmerTomorrow)*100, digits = 2)
cat("Naïve Bayes model accuracy is: ", naive.acc, "%")
wbag_pred = predict.bagging(wbag, test.data)
#confusion matrix
wbag.cfm = wbag_pred$confusion
wbag.acc = round(mean(wbag_pred$class == test.data$WarmerTomorrow)*100, digits = 2)
cat("Bagging ensemble model accuracy is: ", wbag.acc, "%")
wboost_pred = predict.boosting(wboost, test.data)
#confusion matrix
wboost.cfm = wboost_pred$confusion
wboost.acc = round(mean(wboost_pred$class == test.data$WarmerTomorrow)*100, digits = 2)
cat("Boosting ensemble model accuracy is: ", wboost.acc, "%")
wrf_pred = predict(wrf, test.data)
#confusion matrix
wrf.cfm = tree.cfm = table(actual = test.data$WarmerTomorrow, predicted = wrf_pred)
wrf.acc = round(mean(wrf_pred == test.data$WarmerTomorrow)*100, digits = 2)
cat("Random Forest ensemble model accuracy is: ", wrf.acc, "%")
tree.conf = predict(tree.fit, test.data, type = "vector")
tree.conf.pred = prediction(tree.conf[,2], test.data$WarmerTomorrow)
tree.perf = performance(tree.conf.pred, "tpr", "fpr")
plot(tree.perf, main = "ROC curve for classifier") + abline(0,1)
naive.conf = predict(naive.fit, test.data, type = "raw")
naive.conf.pred = prediction(naive.conf[,2], test.data$WarmerTomorrow)
naive.perf = performance(naive.conf.pred, "tpr", "fpr")
plot(naive.perf, col = "blueviolet", add = TRUE)
bag.conf = prediction(wbag_pred$prob[,2], test.data$WarmerTomorrow)
head(bag.conf)
bag.cong
bag.conf
bag.perf = performance(bag.conf, "tpr", "fpr")
plot(bag.perf, col = "green")
plot(bag.perf, col = "green", add = TRUE)
{plot(tree.perf, main = "ROC curve for classifier") + abline(0,1)}
plot(naive.perf, col = "blueviolet", add = TRUE)
plot(tree.perf, main = "ROC curve for classifier")
plot(naive.perf, col = "blueviolet", add = TRUE)
plot(bag.perf, col = "green", add = TRUE)
abline(0,1)
x = plot(tree.perf, main = "ROC curve for classifier") + abline(0,1)
x
x = plot(tree.perf, main = "ROC curve for classifier") + abline(0,1)
plot(naive.perf, col = "blueviolet") + plot(tree.perf, main = "ROC curve for classifier") + abline(0,1)
naive.conf = predict(naive.fit, test.data, type = "raw")
naive.conf.pred = prediction(naive.conf[,2], test.data$WarmerTomorrow)
naive.perf = performance(naive.conf.pred, "tpr", "fpr")
plot(naive.perf, col = "blueviolet", add = TRUE)
tree.conf = predict(tree.fit, test.data, type = "vector")
tree.conf.pred = prediction(tree.conf[,2], test.data$WarmerTomorrow)
tree.perf = performance(tree.conf.pred, "tpr", "fpr")
plot(tree.perf, main = "ROC curve for classifier") + abline(0,1)
naive.conf = predict(naive.fit, test.data, type = "raw")
naive.conf.pred = prediction(naive.conf[,2], test.data$WarmerTomorrow)
naive.perf = performance(naive.conf.pred, "tpr", "fpr")
plot(naive.perf, col = "blueviolet", add = TRUE)
plot(tree.perf, main = "ROC curve for classifier") + abline(0,1)
naive.conf = predict(naive.fit, test.data, type = "raw")
naive.conf.pred = prediction(naive.conf[,2], test.data$WarmerTomorrow)
naive.perf = performance(naive.conf.pred, "tpr", "fpr")
plot(naive.perf, col = "blueviolet", add = TRUE)
plot(naive.perf, col = "blueviolet")
plot(bag.perf, col = "green")
boost.conf = prediction(wboost_pred$prob[,2], test.data$WarmerTomorrow)
boost.perf = performance(boost.conf, "tpr", "fpr")
plot(boost.perf, col ="red")
plot(boost.perf, col ="red") + abline(0,1)
plot(bag.perf, col = "green") + abline(0,1)
plot(naive.perf, col = "blueviolet") + abline(0,1)
plot(naive.perf, col = "blueviolet",  add=TRUE) + abline(0,1)
plot(naive.perf, col = "blueviolet") + abline(0,1)
plot(tree.perf, main = "ROC curve for classifier") + abline(0,1)
plot(tree.perf, main = "ROC curve for classifier", colorize = TRUE) + abline(0,1)
plot(naive.perf, add = TRUE, colorize = TRUE)
plot(tree.perf, col = "black") + abline(0,1)
plot(naive.perf, col = "blueviolet", add = TRUE)
knitr::opts_chunk$set(error=TRUE)
naive.conf = predict(naive.fit, test.data, type = "raw")
naive.conf.pred = prediction(naive.conf[,2], test.data$WarmerTomorrow)
naive.perf = performance(naive.conf.pred, "tpr", "fpr")
plot(naive.perf, col = "blueviolet", add = TRUE)
plot(naive.perf, col = "blueviolet", add = TRUE)
plot(tree.perf, col = "black")
plot(naive.perf, col = "blueviolet", add = TRUE)
plot(tree.perf, col = "black", add  = TRUE)
plot(tree.perf, col = "black") + abline(0,1)
plot(naive.perf, col = "blueviolet") + abline(0,1)
tree.pred = predict(tree.fit, test.data, type = "class")
#confusion matrix
tree.cfm = table(actual = test.data$WarmerTomorrow, predicted = tree.pred)
tree.acc = round(mean(tree.pred == test.data$WarmerTomorrow)*100, digits = 2)
cat("Decision Tree accuracy is: ", tree.acc, "%")
rf.conf = predict(rf.fit, test.data, type = "vector")
#load libraries
library(pastecs)
library(tree)
library(rpart)
library(e1071)
library(adabag)
library(randomForest)
library(ROCR)
library(sf)
rm(list = ls())
WAUS <- read.csv("WarmerTomorrow2022.csv", stringsAsFactors = T)
L <- as.data.frame(c(1:49))
set.seed(31842305) # Your Student ID is the random seed
L <- L[sample(nrow(L), 10, replace = FALSE),] # sample 10 locations
WAUS <- WAUS[(WAUS$Location %in% L),]
WAUS <- WAUS[sample(nrow(WAUS), 2000, replace = FALSE),] # sample 2000 rows
WAUS = na.omit(WAUS)
WAUS$WarmerTomorrow = as.factor(WAUS$WarmerTomorrow)
warm_tmr = as.data.frame(table(WAUS$WarmerTomorrow))
warmer =  round((warm_tmr[2,2] / sum(warm_tmr[,2])) * 100, digits = 2) #52.43
cooler = round((warm_tmr[1,2] / sum(warm_tmr[,2])) * 100, digits = 2) # 47.57
df_summary <- subset(WAUS, select = -c(1,2,3,4,10,12,13,24))
df_summary = round(stat.desc(df_summary), digits = 2)
df_summary
set.seed(31842305)
train.row = sample(1:nrow(WAUS), 0.7*nrow(WAUS))
train.data = WAUS[train.row,]
test.data = WAUS[-train.row,]
tree.fit = tree(WarmerTomorrow ~., data = train.data)
naive.fit = naiveBayes(WarmerTomorrow ~., data = train.data)
wbag.fit = bagging(WarmerTomorrow ~., train.data, mfinal = 10)
wboost.fit = boosting(WarmerTomorrow ~., train.data, mfinal = 10)
rf.fit = randomForest(WarmerTomorrow ~., train.data)
tree.pred = predict(tree.fit, test.data, type = "class")
#confusion matrix
tree.cfm = table(actual = test.data$WarmerTomorrow, predicted = tree.pred)
tree.acc = round(mean(tree.pred == test.data$WarmerTomorrow)*100, digits = 2)
cat("Decision Tree accuracy is: ", tree.acc, "%")
naive.pred = predict(naive.fit, test.data)
#confusion matrix
naive.cfm = table(actual = test.data$WarmerTomorrow, predicted = naive.pred)
naive.acc = round(mean(naive.pred == test.data$WarmerTomorrow)*100, digits = 2)
cat("Naïve Bayes model accuracy is: ", naive.acc, "%")
wbag.fit_pred = predict.bagging(wbag.fit, test.data)
#confusion matrix
wbag.fit.cfm = wbag.fit_pred$confusion
wbag.fit.acc = round(mean(wbag.fit_pred$class == test.data$WarmerTomorrow)*100, digits = 2)
cat("Bagging ensemble model accuracy is: ", wbag.fit.acc, "%")
wboost.fit_pred = predict.boosting(wboost.fit, test.data)
#confusion matrix
wboost.fit.cfm = wboost.fit_pred$confusion
wboost.fit.acc = round(mean(wboost.fit_pred$class == test.data$WarmerTomorrow)*100, digits = 2)
cat("Boosting ensemble model accuracy is: ", wboost.fit.acc, "%")
rf.fit_pred = predict(rf.fit, test.data)
#confusion matrix
rf.fit.cfm = tree.cfm = table(actual = test.data$WarmerTomorrow, predicted = rf.fit_pred)
rf.fit.acc = round(mean(rf.fit_pred == test.data$WarmerTomorrow)*100, digits = 2)
cat("Random Forest ensemble model accuracy is: ", rf.fit.acc, "%")
tree.conf = predict(tree.fit, test.data, type = "vector")
tree.conf.pred = prediction(tree.conf[,2], test.data$WarmerTomorrow)
tree.perf = performance(tree.conf.pred, "tpr", "fpr")
plot(tree.perf, col = "black") + abline(0,1)
naive.conf = predict(naive.fit, test.data, type = "raw")
naive.conf.pred = prediction(naive.conf[,2], test.data$WarmerTomorrow)
naive.perf = performance(naive.conf.pred, "tpr", "fpr")
plot(naive.perf, col = "blueviolet") + abline(0,1)
bag.conf = prediction(wbag.fit_pred$prob[,2], test.data$WarmerTomorrow)
bag.perf = performance(bag.conf, "tpr", "fpr")
plot(bag.perf, col = "green") + abline(0,1)
boost.conf = prediction(wboost.fit_pred$prob[,2], test.data$WarmerTomorrow)
boost.perf = performance(boost.conf, "tpr", "fpr")
plot(boost.perf, col ="red") + abline(0,1)
rf.conf = predict(rf.fit, test.data, type = "vector")
tree.conf = predict(tree.fit, test.data, type = "prob")
tree.conf = predict(tree.fit, test.data, type = "vector")
tree.conf.pred = prediction(tree.conf[,2], test.data$WarmerTomorrow)
tree.perf = performance(tree.conf.pred, "tpr", "fpr")
plot(tree.perf, col = "black") + abline(0,1)
rf.conf = predict(rf.fit, test.data, type = "prob")
rf.conf.pred = prediction(rf.conf[,2], test.data$WarmerTomorrow)
rf.perf = performance(rf.conf.pred, "tpr", "fpr")
plot(rf.perf, col = "cyan")
plot(rf.perf, col = "cyan") + abline(0,1)
plot(rf.perf, col = "cyan", add = TRUE) + abline(0,1)
plot(rf.perf, col = "cyan") + abline(0,1)
#load libraries
library(pastecs)
library(tree)
library(rpart)
library(e1071)
library(adabag)
library(randomForest)
library(ROCR)
library(sf)
rm(list = ls())
WAUS <- read.csv("WarmerTomorrow2022.csv", stringsAsFactors = T)
L <- as.data.frame(c(1:49))
set.seed(31842305) # Your Student ID is the random seed
L <- L[sample(nrow(L), 10, replace = FALSE),] # sample 10 locations
WAUS <- WAUS[(WAUS$Location %in% L),]
WAUS <- WAUS[sample(nrow(WAUS), 2000, replace = FALSE),] # sample 2000 rows
WAUS = na.omit(WAUS)
WAUS$WarmerTomorrow = as.factor(WAUS$WarmerTomorrow)
warm_tmr = as.data.frame(table(WAUS$WarmerTomorrow))
warmer =  round((warm_tmr[2,2] / sum(warm_tmr[,2])) * 100, digits = 2) #52.43
cooler = round((warm_tmr[1,2] / sum(warm_tmr[,2])) * 100, digits = 2) # 47.57
df_summary <- subset(WAUS, select = -c(1,2,3,4,10,12,13,24))
df_summary = round(stat.desc(df_summary), digits = 2)
df_summary
set.seed(31842305)
train.row = sample(1:nrow(WAUS), 0.7*nrow(WAUS))
train.data = WAUS[train.row,]
test.data = WAUS[-train.row,]
tree.fit = tree(WarmerTomorrow ~., data = train.data)
naive.fit = naiveBayes(WarmerTomorrow ~., data = train.data)
wbag.fit = bagging(WarmerTomorrow ~., train.data, mfinal = 10)
wboost.fit = boosting(WarmerTomorrow ~., train.data, mfinal = 10)
rf.fit = randomForest(WarmerTomorrow ~., train.data)
tree.pred = predict(tree.fit, test.data, type = "class")
#confusion matrix
tree.cfm = table(actual = test.data$WarmerTomorrow, predicted = tree.pred)
tree.acc = round(mean(tree.pred == test.data$WarmerTomorrow)*100, digits = 2)
cat("Decision Tree accuracy is: ", tree.acc, "%")
naive.pred = predict(naive.fit, test.data)
#confusion matrix
naive.cfm = table(actual = test.data$WarmerTomorrow, predicted = naive.pred)
naive.acc = round(mean(naive.pred == test.data$WarmerTomorrow)*100, digits = 2)
cat("Naïve Bayes model accuracy is: ", naive.acc, "%")
wbag.fit_pred = predict.bagging(wbag.fit, test.data)
#confusion matrix
wbag.fit.cfm = wbag.fit_pred$confusion
wbag.fit.acc = round(mean(wbag.fit_pred$class == test.data$WarmerTomorrow)*100, digits = 2)
cat("Bagging ensemble model accuracy is: ", wbag.fit.acc, "%")
wboost.fit_pred = predict.boosting(wboost.fit, test.data)
#confusion matrix
wboost.fit.cfm = wboost.fit_pred$confusion
wboost.fit.acc = round(mean(wboost.fit_pred$class == test.data$WarmerTomorrow)*100, digits = 2)
cat("Boosting ensemble model accuracy is: ", wboost.fit.acc, "%")
rf.fit_pred = predict(rf.fit, test.data)
#confusion matrix
rf.fit.cfm = tree.cfm = table(actual = test.data$WarmerTomorrow, predicted = rf.fit_pred)
rf.fit.acc = round(mean(rf.fit_pred == test.data$WarmerTomorrow)*100, digits = 2)
cat("Random Forest ensemble model accuracy is: ", rf.fit.acc, "%")
tree.conf = predict(tree.fit, test.data, type = "vector")
tree.conf.pred = prediction(tree.conf[,2], test.data$WarmerTomorrow)
tree.perf = performance(tree.conf.pred, "tpr", "fpr")
plot(tree.perf, col = "black") + abline(0,1)
naive.conf = predict(naive.fit, test.data, type = "raw")
naive.conf.pred = prediction(naive.conf[,2], test.data$WarmerTomorrow)
naive.perf = performance(naive.conf.pred, "tpr", "fpr")
plot(naive.perf, add = TRUE,  col = "blueviolet")
plot(tree.perf, col = "black")
plot(naive.perf, add = TRUE,  col = "blueviolet")
library("XML")
library("methods")
library("xml2")
library("dplyr")
library("tidyr")
library("readr")
# Convert the input xml file to a data frame.
data = read_xml("article.xml")%>% as_list()
setwd("C:/Users/User/Desktop/FIT3164/allofplos_xml")
# Convert the input xml file to a data frame.
data = read_xml("article.xml")%>% as_list()
print(data)
driver_tb1 = tibble::as.tibble(data) %>% unnest_longer('article')
print(driver_tb1)
df_data <-driver_tb1 %>% unnest_wider('article')
# this line still having error
df_driver = df_data %>% unnest(cols = names(.)) %>% unnest(cols = names(.)) %>% readr::type_convert()
